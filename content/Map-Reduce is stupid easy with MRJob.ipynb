{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You have hundreds of gigabytes of data and aren't sure how to search through it for insights. You know about Amazon Web Services and even store your log files in S3. Maybe you've played with Amazon's Elastic Map Reduce service in the past? Dabbled in Hive but got tired of all of the ceremony? \n",
      "\n",
      "Enter MRJob or Mr. Job as I affectionately refer to it.\n",
      "\n",
      "In this post I'm going to show how to run a very simple job and introduce all of the basic concepts you will need to know. In a future post, I'll dig into some of the more \"advanced\" features and this will be required reading.\n",
      "\n",
      "To start off, Mr. Job is a Python module that makes running EMR tasks on Amazon's infrastructure disgustingly easy. How easy? Let's whet your whistle. The following script counts the number of times every word in your data occurred."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"The classic MapReduce job: count the frequency of words.\n",
      "\"\"\"\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "\n",
      "WORD_RE = re.compile(r\"[\\w']+\")\n",
      "\n",
      "\n",
      "class MRWordFreqCount(MRJob):\n",
      "\n",
      "    def mapper(self, _, line):\n",
      "        for word in WORD_RE.findall(line):\n",
      "            yield (word.lower(), 1)\n",
      "\n",
      "    def reducer(self, word, counts):\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    MRWordFreqCount.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's all it takes and you're up and running. Interested? Keep reading."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# What's in a job?\n",
      "\n",
      "Let's go through the above job and discuss how all of the moving parts come together.\n",
      "\n",
      "## Running a job\n",
      "\n",
      "The first thing to notice is that the file just contains a class and it runs if it's ran directly. How does one invoke it?\n",
      "\n",
      "The simplest way is to invoke a local run of your job. This is something that's much harder on Hive or the like and it can greatly improve your feedback loop as you debug your query. This is how to invoke a run locally:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "python my_mapreduce_job.py sample_data.txt -r local"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By default the results will stream out to your terminal window. I've found that this normally is fine for debugging. This is a huge benefit to anyone who's ever needed to do something simple on Hive. So often so much of your time can just be sunk into debugging your queries and the feedback loop is so loose. You can lose half a day to a day just waiting for your debug results. So run local and often. Fail fast."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When you're done testing and want to run your job on EMR it looks something like this:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "python my_mapreduce_job.py s3://honey-bucket/2013-11-* --output-dir s3://results/ --no-output -r emr --conf-path ~/secrets/mrjob.config"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To point it to S3 all we have to do is give it an S3 path instead of a local one. AND! You can use some patterned file matching to query over your files. Where you see I have \"s3://honey-bucket/2013-11-*\" above I'm basically requesting all files from November of 2013. Also, since we're probably dealing with a much larger amount of data now that we're running on EMR we can have the results output to S3 and prevent Mr. Job from printing the results in the terminal window.\n",
      "\n",
      "\"-r emr\" tells Mr. Job to run this job on EMR. \"--conf-path ~/secrets/mrjob.config\" points to where I store my AWS token and secret key as well as what kind and how many instances should be started."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "My configuration looks something like this:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "runners:\n",
      "  emr:\n",
      "    aws_access_key_id: [my access key]\n",
      "    aws_secret_access_key: [my secret]\n",
      "    ec2_key_pair: some-keypair\n",
      "    ec2_key_pair_file: ~/.ssh/id_rsa.pub # ~/ and $ENV_VARS allowed here\n",
      "    ssh_tunnel_to_job_tracker: true\n",
      "    ec2_instance_type: c1.xlarge\n",
      "    num_ec2_instances: 5"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Mapper\n",
      "\n",
      "Once you run Mr. Job it rips through all of the text you've pointed it to line by line. Each line is fed into the mapper method on your Mr. Job job class. Upon receiving a line of text you can do any you normally would in Python (with some restrictions). With the default configuration the mapper should return a JSON encodeable key and value. That means your key can be a JSON object as can your value.\n",
      "\n",
      "When yielding the key/value pair define your key in a way that you want your values to be grouped into a single list... because that's what is passed to the reducer.\n",
      "\n",
      "So in the case of building a word counter like above, you see that the key that is chosen is a single word and the number one is the value associated (since we only yield one word at a time."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Reducer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In a magic spot within MRJob/Hadoop your each of your keys will have their associated values grouped with them and set by set they will be provided to your reducer. In the case of the word counter then, you will see that we receive a word and an iterable of values. Each of the values will be the number one since MRJob doesn't do anything besides group them automatically.\n",
      "\n",
      "That means we just need to add them all up and yield them from our reducer. Here's that mapper and reducer one more time:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    def mapper(self, _, line):\n",
      "        for word in WORD_RE.findall(line):\n",
      "            yield (word.lower(), 1)\n",
      "\n",
      "    def reducer(self, word, counts):\n",
      "        yield (word, sum(counts))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Under the hood a bit\n",
      "\n",
      "\n",
      "If you're getting into Hadoop I'm sure you understand that the whole point is that jobs are run across many computers and all of this data has to be transferred from computer to computer as it's processed. The reason that the default return value for the above methods requires your data to be JSON-encodeable is so it's possible to serialize the data to a file and ship it off elsewhere to be further processed. There is a concept known as \"Protocols\" that I won't go into in this post but you should seek out if you find yourself needing a different serialization method.\n",
      "\n",
      "Another concept you're sure to run across is something called \"Combiner\". The sole purpose of a combiner is to reduce network usage. Think about it like this. Imagine the mapper method above creating a file where every instance of every word in all of my log files is inside. That'd be huge right? Basically near the size of my log files with the other data stripped out. But now imagine if I ran a local reduce task that only handled the data on the local computer and THEN sent the results across the network to the computer actually running the full multi-node reducer. Now instead of needing to have thousands of lines for each and every word, I will just have one line for each word with the number of occurences that were found on the current server. Then on the computer doing the real reducing, it can roll up these intermediate results as well.\n",
      "\n",
      "Here's the above scenario but with a combiner added:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"The classic MapReduce job: count the frequency of words.\n",
      "\"\"\"\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "\n",
      "WORD_RE = re.compile(r\"[\\w']+\")\n",
      "\n",
      "\n",
      "class MRWordFreqCount(MRJob):\n",
      "\n",
      "    def mapper(self, _, line):\n",
      "        for word in WORD_RE.findall(line):\n",
      "            yield (word.lower(), 1)\n",
      "\n",
      "    def combiner(self, word, counts):\n",
      "        yield (word, sum(counts))\n",
      "            \n",
      "    def reducer(self, word, counts):\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    MRWordFreqCount.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "MRJob takes care of the details and makes sure these methods are all called at the right times in the job flow."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Caveats and advanced topics\n",
      "\n",
      "## Joining?\n",
      "It's not all roses and gum drops though. One concept that MRJob does NOT help very well is in joining different data sources. This will be the subject of an entirely separate post at some point. For now, treat this as a good starting point and see what you can get done with it.\n",
      "\n",
      "## Multiple mappers or reducers?\n",
      "MRJob does a GREAT job of making multistep jobs easy. I'll also dive into this in a later blog post. For now refer to the documentation here: http://pythonhosted.org/mrjob/job.html#multi-step-jobs\n",
      "\n",
      "I the meantime, enjoy MRJob!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}